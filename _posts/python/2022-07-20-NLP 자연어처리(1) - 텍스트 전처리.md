---
layout: post  
current: post  
cover:  assets/built/images/python-logo.png  
navigation: True  
title: NLP 자연어처리(1) - 텍스트 전처리  
date: 2022-07-20 20:00:00 +0900  
tags: [deeplearning, python]  
class: post-template  
subclass: 'post tag-python'  
author: chanp5660  
---

{% include NLP-table-of-contents.html %}

- 워크 플로우 

<img src="https://wikidocs.net/images/page/31947/%EB%A8%B8%EC%8B%A0_%EB%9F%AC%EB%8B%9D_%EC%9B%8C%ED%81%AC%ED%94%8C%EB%A1%9C%EC%9A%B0.PNG" width="100%">  

\* 내용 및 이미지 참조 링크 [https://wikidocs.net/31947](https://wikidocs.net/31947)  

본 블로그 내용은 워크 플로우에서 **"전처리 및 정제"** 부분에 해당 된다.

# NLP 자연어처리(1) - 텍스트 전처리

- 본 블로그는 위키독스에 나와있는 [**딥 러닝을 이용한 자연어 처리 입문**](https://wikidocs.net/book/2155)을 따라하고 실행하면서 나만의 추가 정보들을 넣어가면 진행하기 위한 글이다. 배우려고 하는 사람은 제공된 링크를 타고 가서 배우는게 본 블로그보다 정리가 잘되어 있어 깔끔하고 좋다.

- 나만의 해석 
    - 모든 부분들에서 잘하면 좋겠지만 "텍스트 전처리" 부분은 자연어 처리에서 **가장 중요**하다고 생각한다. 이 부분에서 결과가 만족되지 못한다면 다음 모든 부분에서 절대 만족할 수 없을 것이다.

- 텍스트 전처리(Text Preprocessing) [https://wikidocs.net/21694](https://wikidocs.net/21694)  
    - <a href="#Tokenization">토큰화(Tokenization)</a>  
    - <a href="#Cleaning_Normalization">정제(Cleaning) and 정규화(Normalization)</a>  
    - <a href="#Stemming_Lemmatization">어간 추출(Stemming) and 표제어 추출(Lemmatization)</a>  
    - <a href="#Stopword">불용어(Stopword)</a>
    - <a href="#Regular_Expression">정규 표현식(Regular Expression)</a>
    - <a href="#Integer_Encoding">정수 인코딩(Integer Encoding)</a>
    - <a href="#Padding">패딩(Padding)</a>
    - <a href="#One-Hot_Encoding">원-핫 인코딩(One-Hot Encoding)</a>
    - <a href="#Splitting_Data">데이터의 분리(Splitting Data)</a>
    - <a href="#Text_Preprocessing_Tools_for_Korean_Text">한국어 전처리 패키지(Text Preprocessing Tools for Korean Text)</a>

<p id="Tokenization"></p>  


## 토큰화(Tokenization)

https://wikidocs.net/21698

- 주어진 코퍼스(corpus)에서 토큰(token)이라고 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 한다.
- 코퍼스(corpus) : 말뭉치; 글자들이 묶여있는 단락, 문단 등이라고 생각한다.
- 토큰(token) : 상황에 따라 다르지만 의미 있는 부분을 말한다.

### _ 1. 단어 토큰화(Word Tokenization)  

- 단어를 기준으로 토큰화 작업을 하는 것
- 간단한 예시
    - "토큰화! 작업을 하는 것은, 보통 사람마다 기준이 다릅니다."
    - "토큰화", "작업을", "하는", "것은", "보통", "사람마다", "기준이", "다릅니다"


### _ 2. 토큰화 중 생기는 선택의 순간

- 어떤 용도로 사용할건지에 따라 토큰화할 기준을 직접 정해야한다.

- 간단한 예시
    - Don't에 아포스트로피(')가 있는데 어떤 기준으로 토큰화 하는지에 따른 여러 결과가 있다.
      - Don't, Don t, Dont, Do n't ( 단어 기준 )
      - Jone's, Jone s, Jone, Jones ( 해석 기준 : 가르키는 대상 )
      
- 기존에 공개된 도구들이 존재하면 이용하는게 좋다. 혹시 원하는 작업이 없다면 직접 설계하여 추가하는 방법도 있다.  

- 예시 [https://wikidocs.net/21698](https://wikidocs.net/21698)
    - NLTK : 영어 코퍼스를 토큰화 하기 위한 도구
    ```python
    import nltk
    nltk.download('punkt') # 처음 하시는 분은 실행해 놔야합니다.
    ```


```python
from nltk.tokenize import word_tokenize # 도구 토큰화1
from nltk.tokenize import WordPunctTokenizer # 도구 토큰화2
from tensorflow.keras.preprocessing.text import text_to_word_sequence # 도구 토큰화3
```


```python
print('단어 토큰화1 :',word_tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
```

    단어 토큰화1 : ['Do', "n't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', "'s", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']
    


```python
print('단어 토큰화2 :',WordPunctTokenizer().tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
```

    단어 토큰화2 : ['Don', "'", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', "'", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']
    


```python
print('단어 토큰화3 :',text_to_word_sequence("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
```

    단어 토큰화3 : ["don't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', "jone's", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']
    

- 관찰
    - word_tokenize는 Don't를 Do와 n't로 분리하였으며, 반면 Jone's는 Jone과 's로 분리한 것을 확인
    - WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 가지고 있어 Don't를 Don과 '와 t로 분리, Jone's를 Jone과 '와 s로 분리한 것을 확인
    - text_to_word_sequence(케라스 도구) 에서는 모든 영문을 소문자로 바꾸고 마침표나 컴마, 느낌표 등의 구두점을 제거하지만 단어내에 있는 Don't와 jone's와 같은 경우 아포스트로피는 존재한다.


### _ 3. 토큰화에서 고려해야할 사항  

- 어떻게 해결해야 할지는 정해진 것이 아니라 본인의 목적에 따라 결정해야한다.

1. 구두점이나 특수 문자를 단순 제외해서는 안 된다.  
2. 줄임말과 단어 내에 띄어쓰기가 있는 경우.

- 표준 토큰화 예제  
    Penn Treebank Tokenization의 규칙에 대해서 소개
    - 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.
    - 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다.


```python
from nltk.tokenize import TreebankWordTokenizer

tokenizer = TreebankWordTokenizer()

text = "Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own."

print(text)
print()
print('트리뱅크 워드토크나이저\n',tokenizer.tokenize(text))
```

    Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.
    
    트리뱅크 워드토크나이저
     ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', "n't", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']
    

- 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.
    - 'home-based'
- 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다.
    - 'does', "n't"

### _ 4. 문장 토큰화(Sentence Tokenization)  

- 보통 정제되지 않은 코퍼스 내에서 문장 단위로 구분할 때 사용된다.
- 어떻게 구분할까?
    - 특수문자(., ?, !, 등) 으로 구분한다면?
        - 문장의 끝이 아닌 경우도 많다. ex) aaa@naver.com, ph.D, i'm
        - 따라서 특수문자로만 구분하는 것은 어려운 일이다.

- 제공하는 도구
    - NLTK : 영어 문장의 토큰화 pip install NLTK
    - KSS : 한국어 문장의 토큰화 pip install KSS




```python
from nltk.tokenize import sent_tokenize

text = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near."
print('문장 토큰화1 :',sent_tokenize(text))
```

    문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']
    


```python
text = "I am actively looking for Ph.D. students. and you are a Ph.D student."
print('문장 토큰화2 :',sent_tokenize(text))
```

    문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']
    


```python
import kss

text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 즉, 이제. 해보면 알걸요?'
print('한국어 문장 토큰화 :',kss.split_sentences(text))
```

    한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '즉, 이제. 해보면 알걸요?']
    

- 관찰
    - 문장 토큰화1 : \["Finally,", "about,"\] 처럼 쉼표를 문장구분으로 인정하지 않는다.
    - 문장 토큰화2 ; \["ph.D.", "ph.D"\] 처럼 온점(.)을 문장구분으로 인정하지 않는다.
    - KSS : \["즉, 이제. 해보면 알걸요?"\] 위 두개 처럼 쓰임은 다르지만 올바르게 구분해주는 것을 볼 수 있다.

### _ 5. 한국어에서의 토큰화의 어려움

- 

<p id="Cleaning_Normalization"></p>  


## 정제(Cleaning) and 정규화(Normalization)

https://wikidocs.net/21693

<p id="Stemming_Lemmatization"></p>  


## 어간 추출(Stemming) and 표제어 추출(Lemmatization)

https://wikidocs.net/21707

<p id="Stopword"></p>  


## 불용어(Stopword)

https://wikidocs.net/22530

<p id="Regular_Expression"></p>  


## 정규 표현식(Regular Expression)

https://wikidocs.net/21703

<p id="Integer_Encoding"></p>  


## 정수 인코딩(Integer Encoding)

https://wikidocs.net/31766

<p id="Padding"></p>  


## 패딩(Padding)

https://wikidocs.net/83544

<p id="One-Hot_Encoding"></p>  


## 원-핫 인코딩(One-Hot Encoding)

https://wikidocs.net/22647

<p id="Splitting_Data"></p>  


## 데이터의 분리(Splitting Data)

https://wikidocs.net/33274

<p id="Text_Preprocessing_Tools_for_Korean_Text"></p>  


## 한국어 전처리 패키지(Text Preprocessing Tools for Korean Text)

https://wikidocs.net/92961
